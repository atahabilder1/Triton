/home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'pyg-lib'. Disabling its usage. Stacktrace: Could not load this library: /home/anik/code/Triton/triton_env/lib/python3.12/site-packages/libpyg.so
  import torch_geometric.typing
/home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-scatter'. Disabling its usage. Stacktrace: Could not load this library: /home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_scatter/_version_cuda.so
  import torch_geometric.typing
/home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-cluster'. Disabling its usage. Stacktrace: Could not load this library: /home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_cluster/_version_cuda.so
  import torch_geometric.typing
/home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-spline-conv'. Disabling its usage. Stacktrace: Could not load this library: /home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_spline_conv/_version_cuda.so
  import torch_geometric.typing
/home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_geometric/__init__.py:4: UserWarning: An issue occurred while importing 'torch-sparse'. Disabling its usage. Stacktrace: Could not load this library: /home/anik/code/Triton/triton_env/lib/python3.12/site-packages/torch_sparse/_version_cuda.so
  import torch_geometric.typing
2025-10-30 03:22:33,331 - INFO - ================================================================================
2025-10-30 03:22:33,331 - INFO - TRITON TRAINING PIPELINE
2025-10-30 03:22:33,331 - INFO - ================================================================================
2025-10-30 03:22:33,331 - INFO - Training directory: data/datasets/smartbugs-curated/dataset
2025-10-30 03:22:33,331 - INFO - Output directory: models/checkpoints
2025-10-30 03:22:33,331 - INFO - Batch size: 4
2025-10-30 03:22:33,331 - INFO - Epochs: 5
2025-10-30 03:22:33,331 - INFO - Learning rate: 0.001
2025-10-30 03:22:33,331 - INFO - 
Loading dataset...
2025-10-30 03:22:33,331 - INFO - Loading contracts from data/datasets/smartbugs-curated/dataset...
2025-10-30 03:22:33,331 - INFO - Found 18 access_control contracts
2025-10-30 03:22:33,332 - INFO - Found 15 arithmetic contracts
2025-10-30 03:22:33,332 - INFO - Found 8 bad_randomness contracts
2025-10-30 03:22:33,332 - INFO - Found 6 denial_of_service contracts
2025-10-30 03:22:33,332 - INFO - Found 4 front_running contracts
2025-10-30 03:22:33,332 - INFO - Found 31 reentrancy contracts
2025-10-30 03:22:33,332 - INFO - Found 1 short_addresses contracts
2025-10-30 03:22:33,332 - INFO - Found 5 time_manipulation contracts
2025-10-30 03:22:33,332 - INFO - Found 52 unchecked_low_level_calls contracts
2025-10-30 03:22:33,333 - INFO - Found 3 other contracts
2025-10-30 03:22:33,333 - INFO - Loaded 143 contracts total
2025-10-30 03:22:33,333 - INFO - Label distribution:
2025-10-30 03:22:33,333 - INFO -   access_control: 18 contracts
2025-10-30 03:22:33,333 - INFO -   arithmetic: 15 contracts
2025-10-30 03:22:33,333 - INFO -   bad_randomness: 8 contracts
2025-10-30 03:22:33,333 - INFO -   denial_of_service: 6 contracts
2025-10-30 03:22:33,333 - INFO -   front_running: 4 contracts
2025-10-30 03:22:33,333 - INFO -   reentrancy: 31 contracts
2025-10-30 03:22:33,333 - INFO -   short_addresses: 1 contracts
2025-10-30 03:22:33,333 - INFO -   time_manipulation: 5 contracts
2025-10-30 03:22:33,333 - INFO -   unchecked_low_level_calls: 52 contracts
2025-10-30 03:22:33,333 - INFO -   other: 3 contracts
2025-10-30 03:22:33,333 - INFO - Training samples: 114
2025-10-30 03:22:33,333 - INFO - Validation samples: 29
2025-10-30 03:22:33,333 - INFO - Using device: cuda
2025-10-30 03:22:33,333 - INFO - Initializing models...
Some weights of RobertaModel were not initialized from the model checkpoint at microsoft/graphcodebert-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
2025-10-30 03:22:34,476 - INFO - 
Starting training...
2025-10-30 03:22:34,476 - INFO - ================================================================================
2025-10-30 03:22:34,476 - INFO - PHASE 1: Fine-tuning Semantic Encoder (GraphCodeBERT)
2025-10-30 03:22:34,476 - INFO - ================================================================================
2025-10-30 03:22:34,476 - INFO - 
Epoch 1/5
Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=2.31]Training Semantic Encoder:   3%|▎         | 1/29 [00:00<00:06,  4.32it/s, loss=2.31]Training Semantic Encoder:   3%|▎         | 1/29 [00:00<00:06,  4.32it/s, loss=2.3] Training Semantic Encoder:   3%|▎         | 1/29 [00:00<00:06,  4.32it/s, loss=2.29]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:03,  7.49it/s, loss=2.29]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:03,  7.49it/s, loss=2.29]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:03,  7.77it/s, loss=2.29]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:03,  7.77it/s, loss=2.28]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:03,  7.77it/s, loss=2.28]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02,  9.24it/s, loss=2.28]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02,  9.24it/s, loss=2.32]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02,  9.24it/s, loss=2.3] Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.04it/s, loss=2.3]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.04it/s, loss=2.25]Training Semantic Encoder:  28%|██▊       | 8/29 [00:01<00:01, 11.04it/s, loss=2.26]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 10.24it/s, loss=2.26]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 10.24it/s, loss=2.22]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 10.24it/s, loss=2.21]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.80it/s, loss=2.21]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.80it/s, loss=2.25]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.80it/s, loss=2.25]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01,  9.79it/s, loss=2.25]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01,  9.79it/s, loss=2.24]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01,  9.79it/s, loss=2.15]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01,  9.22it/s, loss=2.15]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01,  9.22it/s, loss=2.12]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01,  9.22it/s, loss=2.07]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01,  9.34it/s, loss=2.07]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:02<00:01,  9.34it/s, loss=2.27]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:02<00:01,  9.34it/s, loss=2.17]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.69it/s, loss=2.17]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.69it/s, loss=2.07]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.69it/s, loss=2.3] Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 10.52it/s, loss=2.3]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 10.52it/s, loss=2.04]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 10.52it/s, loss=1.86]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 11.60it/s, loss=1.86]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 11.60it/s, loss=2.18]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 11.60it/s, loss=1.8] Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 10.16it/s, loss=1.8]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 10.16it/s, loss=1.9]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 10.16it/s, loss=1.81]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00, 10.08it/s, loss=1.81]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00, 10.08it/s, loss=2.04]Training Semantic Encoder: 100%|██████████| 29/29 [00:02<00:00,  9.86it/s, loss=2.04]
2025-10-30 03:22:37,612 - INFO - Train Loss: 2.1657, Train Acc: 42.11%
2025-10-30 03:22:37,612 - INFO - Val Loss: 2.0786, Val Acc: 17.24%
2025-10-30 03:22:37,919 - INFO - ✓ Saved best semantic encoder (val_loss: 2.0786)
2025-10-30 03:22:37,919 - INFO - 
Epoch 2/5
Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.98]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.64]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02,  9.34it/s, loss=1.64]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02,  9.34it/s, loss=2.14]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:02,  8.94it/s, loss=2.14]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:02,  8.94it/s, loss=2]   Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:02,  8.94it/s, loss=1.9]Training Semantic Encoder:  17%|█▋        | 5/29 [00:00<00:02, 10.52it/s, loss=1.9]Training Semantic Encoder:  17%|█▋        | 5/29 [00:00<00:02, 10.52it/s, loss=1.9]Training Semantic Encoder:  17%|█▋        | 5/29 [00:00<00:02, 10.52it/s, loss=2.3]Training Semantic Encoder:  24%|██▍       | 7/29 [00:00<00:02,  9.50it/s, loss=2.3]Training Semantic Encoder:  24%|██▍       | 7/29 [00:00<00:02,  9.50it/s, loss=1.77]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:02,  9.12it/s, loss=1.77]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:02,  9.12it/s, loss=2.14]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:02,  9.12it/s, loss=1.84]Training Semantic Encoder:  34%|███▍      | 10/29 [00:00<00:01, 10.94it/s, loss=1.84]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 10.94it/s, loss=1.67]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 10.94it/s, loss=1.97]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.74it/s, loss=1.97]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.74it/s, loss=1.97]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.74it/s, loss=1.96]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01,  9.76it/s, loss=1.96]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01,  9.76it/s, loss=2.03]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01,  9.76it/s, loss=2.18]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.19it/s, loss=2.18]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.19it/s, loss=2.19]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.19it/s, loss=2.17]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01, 10.32it/s, loss=2.17]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01, 10.32it/s, loss=1.91]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01, 10.32it/s, loss=1.68]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:01<00:00, 10.50it/s, loss=1.68]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00, 10.50it/s, loss=1.65]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00, 10.50it/s, loss=1.67]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00,  9.69it/s, loss=1.67]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00,  9.69it/s, loss=2.24]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00,  9.69it/s, loss=1.71]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00,  9.76it/s, loss=1.71]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00,  9.76it/s, loss=1.75]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00,  9.76it/s, loss=2.23]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00,  9.76it/s, loss=2.23]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00,  9.76it/s, loss=2.36]Training Semantic Encoder:  93%|█████████▎| 27/29 [00:02<00:00,  9.50it/s, loss=2.36]Training Semantic Encoder:  93%|█████████▎| 27/29 [00:02<00:00,  9.50it/s, loss=2.17]Training Semantic Encoder:  93%|█████████▎| 27/29 [00:02<00:00,  9.50it/s, loss=2.17]Training Semantic Encoder: 100%|██████████| 29/29 [00:02<00:00, 10.87it/s, loss=2.17]Training Semantic Encoder: 100%|██████████| 29/29 [00:02<00:00, 10.12it/s, loss=2.17]
2025-10-30 03:22:40,975 - INFO - Train Loss: 1.9763, Train Acc: 41.23%
2025-10-30 03:22:40,975 - INFO - Val Loss: 2.0279, Val Acc: 17.24%
2025-10-30 03:22:41,258 - INFO - ✓ Saved best semantic encoder (val_loss: 2.0279)
2025-10-30 03:22:41,258 - INFO - 
Epoch 3/5
Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.87]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.74]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02, 10.10it/s, loss=1.74]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02, 10.10it/s, loss=1.84]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02, 10.10it/s, loss=2.11]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:02, 10.69it/s, loss=2.11]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:02, 10.69it/s, loss=2.07]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:02, 10.69it/s, loss=1.79]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02, 10.76it/s, loss=1.79]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02, 10.76it/s, loss=2.04]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02, 10.76it/s, loss=1.69]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.62it/s, loss=1.69]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.62it/s, loss=1.87]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.62it/s, loss=2.19]Training Semantic Encoder:  34%|███▍      | 10/29 [00:00<00:01, 11.32it/s, loss=2.19]Training Semantic Encoder:  34%|███▍      | 10/29 [00:00<00:01, 11.32it/s, loss=1.94]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 11.32it/s, loss=2.06]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 11.22it/s, loss=2.06]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 11.22it/s, loss=1.8] Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 11.22it/s, loss=1.91]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.15it/s, loss=1.91]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.15it/s, loss=1.86]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.15it/s, loss=1.87]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.41it/s, loss=1.87]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.41it/s, loss=2.06]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.41it/s, loss=2.15]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:00, 11.17it/s, loss=2.15]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:00, 11.17it/s, loss=2.12]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:00, 11.17it/s, loss=1.73]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:01<00:00,  9.96it/s, loss=1.73]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.96it/s, loss=2.03]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.96it/s, loss=1.86]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00,  9.66it/s, loss=1.86]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00,  9.66it/s, loss=2.09]Training Semantic Encoder:  79%|███████▉  | 23/29 [00:02<00:00,  9.40it/s, loss=2.09]Training Semantic Encoder:  79%|███████▉  | 23/29 [00:02<00:00,  9.40it/s, loss=1.84]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00,  9.14it/s, loss=1.84]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00,  9.14it/s, loss=1.88]Training Semantic Encoder:  86%|████████▌ | 25/29 [00:02<00:00,  8.90it/s, loss=1.88]Training Semantic Encoder:  86%|████████▌ | 25/29 [00:02<00:00,  8.90it/s, loss=1.91]Training Semantic Encoder:  86%|████████▌ | 25/29 [00:02<00:00,  8.90it/s, loss=1.89]Training Semantic Encoder:  93%|█████████▎| 27/29 [00:02<00:00,  9.14it/s, loss=1.89]Training Semantic Encoder:  93%|█████████▎| 27/29 [00:02<00:00,  9.14it/s, loss=1.57]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00,  8.89it/s, loss=1.57]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00,  8.89it/s, loss=2.07]Training Semantic Encoder: 100%|██████████| 29/29 [00:02<00:00, 10.10it/s, loss=2.07]
2025-10-30 03:22:44,324 - INFO - Train Loss: 1.9263, Train Acc: 41.23%
2025-10-30 03:22:44,324 - INFO - Val Loss: 1.9715, Val Acc: 17.24%
2025-10-30 03:22:44,611 - INFO - ✓ Saved best semantic encoder (val_loss: 1.9715)
2025-10-30 03:22:44,611 - INFO - 
Epoch 4/5
Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.78]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=2.13]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02,  9.51it/s, loss=2.13]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:02,  9.51it/s, loss=1.88]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:02,  9.09it/s, loss=1.88]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:02,  9.09it/s, loss=2.11]Training Semantic Encoder:  10%|█         | 3/29 [00:00<00:02,  9.09it/s, loss=1.54]Training Semantic Encoder:  17%|█▋        | 5/29 [00:00<00:02,  9.99it/s, loss=1.54]Training Semantic Encoder:  17%|█▋        | 5/29 [00:00<00:02,  9.99it/s, loss=1.77]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02,  9.46it/s, loss=1.77]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02,  9.46it/s, loss=1.53]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:02,  9.46it/s, loss=1.96]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.56it/s, loss=1.96]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.56it/s, loss=1.83]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:01, 11.56it/s, loss=1.71]Training Semantic Encoder:  34%|███▍      | 10/29 [00:00<00:01, 11.03it/s, loss=1.71]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 11.03it/s, loss=2.01]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 11.03it/s, loss=1.83]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01,  9.97it/s, loss=1.83]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01,  9.97it/s, loss=1.97]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01,  9.97it/s, loss=1.75]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.29it/s, loss=1.75]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.29it/s, loss=2.08]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.29it/s, loss=1.83]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01,  9.55it/s, loss=1.83]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01,  9.55it/s, loss=1.57]Training Semantic Encoder:  59%|█████▊    | 17/29 [00:01<00:01,  9.21it/s, loss=1.57]Training Semantic Encoder:  59%|█████▊    | 17/29 [00:01<00:01,  9.21it/s, loss=1.8] Training Semantic Encoder:  59%|█████▊    | 17/29 [00:01<00:01,  9.21it/s, loss=1.69]Training Semantic Encoder:  66%|██████▌   | 19/29 [00:01<00:01,  9.90it/s, loss=1.69]Training Semantic Encoder:  66%|██████▌   | 19/29 [00:02<00:01,  9.90it/s, loss=1.75]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.70it/s, loss=1.75]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.70it/s, loss=1.8] Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00,  9.70it/s, loss=1.73]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 11.58it/s, loss=1.73]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 11.58it/s, loss=1.97]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 11.58it/s, loss=1.89]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 12.53it/s, loss=1.89]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 12.53it/s, loss=1.56]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 12.53it/s, loss=1.73]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 11.68it/s, loss=1.73]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 11.68it/s, loss=1.78]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 11.68it/s, loss=2.24]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00, 10.40it/s, loss=2.24]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00, 10.40it/s, loss=2.08]Training Semantic Encoder: 100%|██████████| 29/29 [00:02<00:00, 10.59it/s, loss=2.08]
2025-10-30 03:22:47,539 - INFO - Train Loss: 1.8382, Train Acc: 41.23%
2025-10-30 03:22:47,539 - INFO - Val Loss: 1.8485, Val Acc: 17.24%
2025-10-30 03:22:47,821 - INFO - ✓ Saved best semantic encoder (val_loss: 1.8485)
2025-10-30 03:22:47,821 - INFO - 
Epoch 5/5
Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.81]Training Semantic Encoder:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.57]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:01, 18.32it/s, loss=1.57]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:01, 18.32it/s, loss=1.62]Training Semantic Encoder:   7%|▋         | 2/29 [00:00<00:01, 18.32it/s, loss=1.63]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:01, 13.04it/s, loss=1.63]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:01, 13.04it/s, loss=1.88]Training Semantic Encoder:  14%|█▍        | 4/29 [00:00<00:01, 13.04it/s, loss=1.71]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:01, 12.36it/s, loss=1.71]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:01, 12.36it/s, loss=2.01]Training Semantic Encoder:  21%|██        | 6/29 [00:00<00:01, 12.36it/s, loss=1.88]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:02, 10.41it/s, loss=1.88]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:02, 10.41it/s, loss=1.57]Training Semantic Encoder:  28%|██▊       | 8/29 [00:00<00:02, 10.41it/s, loss=1.85]Training Semantic Encoder:  34%|███▍      | 10/29 [00:00<00:01, 12.20it/s, loss=1.85]Training Semantic Encoder:  34%|███▍      | 10/29 [00:00<00:01, 12.20it/s, loss=1.77]Training Semantic Encoder:  34%|███▍      | 10/29 [00:01<00:01, 12.20it/s, loss=1.59]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.47it/s, loss=1.59]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.47it/s, loss=1.78]Training Semantic Encoder:  41%|████▏     | 12/29 [00:01<00:01, 10.47it/s, loss=1.53]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.41it/s, loss=1.53]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.41it/s, loss=1.54]Training Semantic Encoder:  48%|████▊     | 14/29 [00:01<00:01, 10.41it/s, loss=1.72]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.34it/s, loss=1.72]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.34it/s, loss=2.21]Training Semantic Encoder:  55%|█████▌    | 16/29 [00:01<00:01, 10.34it/s, loss=1.76]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01,  9.66it/s, loss=1.76]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01,  9.66it/s, loss=1.74]Training Semantic Encoder:  62%|██████▏   | 18/29 [00:01<00:01,  9.66it/s, loss=1.8] Training Semantic Encoder:  69%|██████▉   | 20/29 [00:01<00:00, 11.30it/s, loss=1.8]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:01<00:00, 11.30it/s, loss=1.79]Training Semantic Encoder:  69%|██████▉   | 20/29 [00:02<00:00, 11.30it/s, loss=1.58]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 10.78it/s, loss=1.58]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 10.78it/s, loss=1.58]Training Semantic Encoder:  76%|███████▌  | 22/29 [00:02<00:00, 10.78it/s, loss=1.55]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 10.23it/s, loss=1.55]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 10.23it/s, loss=1.71]Training Semantic Encoder:  83%|████████▎ | 24/29 [00:02<00:00, 10.23it/s, loss=1.98]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 10.57it/s, loss=1.98]Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 10.57it/s, loss=1.7] Training Semantic Encoder:  90%|████████▉ | 26/29 [00:02<00:00, 10.57it/s, loss=1.67]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00, 10.37it/s, loss=1.67]Training Semantic Encoder:  97%|█████████▋| 28/29 [00:02<00:00, 10.37it/s, loss=1.94]Training Semantic Encoder: 100%|██████████| 29/29 [00:02<00:00, 10.86it/s, loss=1.94]
2025-10-30 03:22:50,676 - INFO - Train Loss: 1.7399, Train Acc: 58.77%
2025-10-30 03:22:50,676 - INFO - Val Loss: 1.7563, Val Acc: 37.93%
2025-10-30 03:22:50,955 - INFO - ✓ Saved best semantic encoder (val_loss: 1.7563)
2025-10-30 03:22:50,955 - INFO - 
Semantic Encoder training complete! Best val_loss: 1.7563
2025-10-30 03:22:50,956 - INFO - ================================================================================
2025-10-30 03:22:50,956 - INFO - PHASE 2: Training Fusion Module
2025-10-30 03:22:50,956 - INFO - ================================================================================
2025-10-30 03:22:50,956 - INFO - 
Epoch 1/5
Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=2.3]Training Fusion Module:   3%|▎         | 1/29 [00:00<00:03,  8.13it/s, loss=2.3]Training Fusion Module:   3%|▎         | 1/29 [00:00<00:03,  8.13it/s, loss=2.21]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:03,  8.43it/s, loss=2.21]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:03,  8.43it/s, loss=2.55]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:03,  8.43it/s, loss=1.9] Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  9.23it/s, loss=1.9]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  9.23it/s, loss=2.69]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02,  9.05it/s, loss=2.69]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02,  9.05it/s, loss=1.7] Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02,  9.05it/s, loss=1.87]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02,  9.70it/s, loss=1.87]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02,  9.70it/s, loss=2.05]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02,  9.70it/s, loss=2.41]Training Fusion Module:  31%|███       | 9/29 [00:00<00:02,  9.87it/s, loss=2.41]Training Fusion Module:  31%|███       | 9/29 [00:01<00:02,  9.87it/s, loss=1.94]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:01,  9.54it/s, loss=1.94]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:01,  9.54it/s, loss=2.24]Training Fusion Module:  38%|███▊      | 11/29 [00:01<00:01,  9.24it/s, loss=2.24]Training Fusion Module:  38%|███▊      | 11/29 [00:01<00:01,  9.24it/s, loss=1.41]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.07it/s, loss=1.41]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.07it/s, loss=1.73]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.07it/s, loss=2.66]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01,  9.36it/s, loss=2.66]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01,  9.36it/s, loss=1.85]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  9.28it/s, loss=1.85]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  9.28it/s, loss=1.25]Training Fusion Module:  55%|█████▌    | 16/29 [00:01<00:01,  9.05it/s, loss=1.25]Training Fusion Module:  55%|█████▌    | 16/29 [00:01<00:01,  9.05it/s, loss=1.88]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  8.89it/s, loss=1.88]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  8.89it/s, loss=2.19]Training Fusion Module:  62%|██████▏   | 18/29 [00:01<00:01,  8.77it/s, loss=2.19]Training Fusion Module:  62%|██████▏   | 18/29 [00:02<00:01,  8.77it/s, loss=1.37]Training Fusion Module:  62%|██████▏   | 18/29 [00:02<00:01,  8.77it/s, loss=1.28]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00, 10.35it/s, loss=1.28]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00, 10.35it/s, loss=2.38]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00, 10.35it/s, loss=2.28]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.62it/s, loss=2.28]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.62it/s, loss=1.39]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.62it/s, loss=1.42]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.84it/s, loss=1.42]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.84it/s, loss=1.12]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.84it/s, loss=3.71]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 12.50it/s, loss=3.71]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 12.50it/s, loss=1.71]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 12.50it/s, loss=2.12]Training Fusion Module:  97%|█████████▋| 28/29 [00:02<00:00, 10.99it/s, loss=2.12]Training Fusion Module:  97%|█████████▋| 28/29 [00:02<00:00, 10.99it/s, loss=1.33]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 10.23it/s, loss=1.33]
2025-10-30 03:22:53,985 - INFO - Train Loss: 1.9630, Train Acc: 38.60%
2025-10-30 03:22:53,985 - INFO - Val Loss: 1.7139, Val Acc: 51.72%
2025-10-30 03:22:54,315 - INFO - ✓ Saved all models for epoch 1
2025-10-30 03:22:54,315 - INFO - ✓ Saved best fusion model (val_loss: 1.7139)
2025-10-30 03:22:54,315 - INFO - 
Epoch 2/5
Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.79]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.93]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02, 11.21it/s, loss=1.93]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02, 11.21it/s, loss=1.5] Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02, 11.21it/s, loss=2.7]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  9.63it/s, loss=2.7]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  9.63it/s, loss=1.09]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02,  9.07it/s, loss=1.09]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02,  9.07it/s, loss=3.76]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02,  9.07it/s, loss=1.15]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02,  9.77it/s, loss=1.15]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02,  9.77it/s, loss=0.533]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:02,  9.47it/s, loss=0.533]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:02,  9.47it/s, loss=2.9]  Training Fusion Module:  31%|███       | 9/29 [00:00<00:02,  9.26it/s, loss=2.9]Training Fusion Module:  31%|███       | 9/29 [00:01<00:02,  9.26it/s, loss=1.71]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:02,  9.00it/s, loss=1.71]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:02,  9.00it/s, loss=1.37]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:02,  9.00it/s, loss=1.23]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.65it/s, loss=1.23]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.65it/s, loss=2.61]Training Fusion Module:  45%|████▍     | 13/29 [00:01<00:01,  9.37it/s, loss=2.61]Training Fusion Module:  45%|████▍     | 13/29 [00:01<00:01,  9.37it/s, loss=1.74]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01,  9.16it/s, loss=1.74]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01,  9.16it/s, loss=2.02]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  8.95it/s, loss=2.02]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  8.95it/s, loss=1.48]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  8.95it/s, loss=1.62]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  9.37it/s, loss=1.62]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  9.37it/s, loss=1.79]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  9.37it/s, loss=2.4] Training Fusion Module:  66%|██████▌   | 19/29 [00:01<00:01,  9.77it/s, loss=2.4]Training Fusion Module:  66%|██████▌   | 19/29 [00:02<00:01,  9.77it/s, loss=2.38]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00,  9.45it/s, loss=2.38]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00,  9.45it/s, loss=1.48]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00,  9.45it/s, loss=2.1] Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.48it/s, loss=2.1]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.48it/s, loss=1.7]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.48it/s, loss=1.43]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.57it/s, loss=1.43]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.57it/s, loss=1.92]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.57it/s, loss=1.21]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 12.59it/s, loss=1.21]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 12.59it/s, loss=1.85]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 12.59it/s, loss=1.57]Training Fusion Module:  97%|█████████▋| 28/29 [00:02<00:00, 11.03it/s, loss=1.57]Training Fusion Module:  97%|█████████▋| 28/29 [00:02<00:00, 11.03it/s, loss=1.75]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 10.18it/s, loss=1.75]
2025-10-30 03:22:57,358 - INFO - Train Loss: 1.8172, Train Acc: 34.21%
2025-10-30 03:22:57,358 - INFO - Val Loss: 1.6112, Val Acc: 51.72%
2025-10-30 03:22:57,682 - INFO - ✓ Saved all models for epoch 2
2025-10-30 03:22:57,682 - INFO - ✓ Saved best fusion model (val_loss: 1.6112)
2025-10-30 03:22:57,682 - INFO - 
Epoch 3/5
Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=2.46]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.5] Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02, 13.14it/s, loss=1.5]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02, 13.14it/s, loss=1.47]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02, 13.14it/s, loss=1.39]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02, 10.55it/s, loss=1.39]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02, 10.55it/s, loss=1.57]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02, 10.55it/s, loss=1.25]Training Fusion Module:  21%|██        | 6/29 [00:00<00:02, 10.89it/s, loss=1.25]Training Fusion Module:  21%|██        | 6/29 [00:00<00:02, 10.89it/s, loss=1.02]Training Fusion Module:  21%|██        | 6/29 [00:00<00:02, 10.89it/s, loss=1.32]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:01, 10.52it/s, loss=1.32]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:01, 10.52it/s, loss=2.07]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:01, 10.52it/s, loss=2.47]Training Fusion Module:  34%|███▍      | 10/29 [00:00<00:01, 12.41it/s, loss=2.47]Training Fusion Module:  34%|███▍      | 10/29 [00:00<00:01, 12.41it/s, loss=1.78]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:01, 12.41it/s, loss=1.43]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01, 12.07it/s, loss=1.43]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01, 12.07it/s, loss=1.32]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01, 12.07it/s, loss=1.51]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01, 11.16it/s, loss=1.51]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01, 11.16it/s, loss=1.98]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01, 11.16it/s, loss=2.77]Training Fusion Module:  55%|█████▌    | 16/29 [00:01<00:01, 11.20it/s, loss=2.77]Training Fusion Module:  55%|█████▌    | 16/29 [00:01<00:01, 11.20it/s, loss=1.97]Training Fusion Module:  55%|█████▌    | 16/29 [00:01<00:01, 11.20it/s, loss=1.47]Training Fusion Module:  62%|██████▏   | 18/29 [00:01<00:01, 10.28it/s, loss=1.47]Training Fusion Module:  62%|██████▏   | 18/29 [00:01<00:01, 10.28it/s, loss=1.51]Training Fusion Module:  62%|██████▏   | 18/29 [00:01<00:01, 10.28it/s, loss=1.23]Training Fusion Module:  69%|██████▉   | 20/29 [00:01<00:00, 10.45it/s, loss=1.23]Training Fusion Module:  69%|██████▉   | 20/29 [00:01<00:00, 10.45it/s, loss=1.74]Training Fusion Module:  69%|██████▉   | 20/29 [00:02<00:00, 10.45it/s, loss=0.697]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.28it/s, loss=0.697]Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.28it/s, loss=1.78] Training Fusion Module:  76%|███████▌  | 22/29 [00:02<00:00, 10.28it/s, loss=2.05]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.62it/s, loss=2.05]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.62it/s, loss=2.58]Training Fusion Module:  83%|████████▎ | 24/29 [00:02<00:00, 11.62it/s, loss=2.94]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 11.34it/s, loss=2.94]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 11.34it/s, loss=1.79]Training Fusion Module:  90%|████████▉ | 26/29 [00:02<00:00, 11.34it/s, loss=1.9] Training Fusion Module:  97%|█████████▋| 28/29 [00:02<00:00, 11.38it/s, loss=1.9]Training Fusion Module:  97%|█████████▋| 28/29 [00:02<00:00, 11.38it/s, loss=1.94]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 11.31it/s, loss=1.94]
2025-10-30 03:23:00,437 - INFO - Train Loss: 1.7555, Train Acc: 42.11%
2025-10-30 03:23:00,437 - INFO - Val Loss: 1.8961, Val Acc: 20.69%
2025-10-30 03:23:00,437 - INFO - 
Epoch 4/5
Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=2.01]Training Fusion Module:   3%|▎         | 1/29 [00:00<00:03,  8.63it/s, loss=2.01]Training Fusion Module:   3%|▎         | 1/29 [00:00<00:03,  8.63it/s, loss=1.92]Training Fusion Module:   3%|▎         | 1/29 [00:00<00:03,  8.63it/s, loss=1.37]Training Fusion Module:  10%|█         | 3/29 [00:00<00:02, 12.15it/s, loss=1.37]Training Fusion Module:  10%|█         | 3/29 [00:00<00:02, 12.15it/s, loss=1.92]Training Fusion Module:  10%|█         | 3/29 [00:00<00:02, 12.15it/s, loss=1.44]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02, 11.57it/s, loss=1.44]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02, 11.57it/s, loss=2.29]Training Fusion Module:  17%|█▋        | 5/29 [00:00<00:02, 11.57it/s, loss=2.07]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02, 10.08it/s, loss=2.07]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02, 10.08it/s, loss=2.33]Training Fusion Module:  24%|██▍       | 7/29 [00:00<00:02, 10.08it/s, loss=1.97]Training Fusion Module:  31%|███       | 9/29 [00:00<00:01, 11.43it/s, loss=1.97]Training Fusion Module:  31%|███       | 9/29 [00:00<00:01, 11.43it/s, loss=1.64]Training Fusion Module:  31%|███       | 9/29 [00:01<00:01, 11.43it/s, loss=3.97]Training Fusion Module:  38%|███▊      | 11/29 [00:01<00:01, 10.13it/s, loss=3.97]Training Fusion Module:  38%|███▊      | 11/29 [00:01<00:01, 10.13it/s, loss=3.2] Training Fusion Module:  38%|███▊      | 11/29 [00:01<00:01, 10.13it/s, loss=2.08]Training Fusion Module:  45%|████▍     | 13/29 [00:01<00:01, 10.08it/s, loss=2.08]Training Fusion Module:  45%|████▍     | 13/29 [00:01<00:01, 10.08it/s, loss=1.74]Training Fusion Module:  45%|████▍     | 13/29 [00:01<00:01, 10.08it/s, loss=1.25]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01, 10.39it/s, loss=1.25]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01, 10.39it/s, loss=1.38]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01, 10.39it/s, loss=1.24]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  9.68it/s, loss=1.24]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  9.68it/s, loss=1.82]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01,  9.68it/s, loss=1.81]Training Fusion Module:  66%|██████▌   | 19/29 [00:01<00:00, 10.53it/s, loss=1.81]Training Fusion Module:  66%|██████▌   | 19/29 [00:01<00:00, 10.53it/s, loss=1.33]Training Fusion Module:  66%|██████▌   | 19/29 [00:01<00:00, 10.53it/s, loss=2.97]Training Fusion Module:  72%|███████▏  | 21/29 [00:01<00:00, 11.22it/s, loss=2.97]Training Fusion Module:  72%|███████▏  | 21/29 [00:02<00:00, 11.22it/s, loss=3.59]Training Fusion Module:  72%|███████▏  | 21/29 [00:02<00:00, 11.22it/s, loss=2.09]Training Fusion Module:  79%|███████▉  | 23/29 [00:02<00:00, 10.30it/s, loss=2.09]Training Fusion Module:  79%|███████▉  | 23/29 [00:02<00:00, 10.30it/s, loss=2.12]Training Fusion Module:  79%|███████▉  | 23/29 [00:02<00:00, 10.30it/s, loss=2.17]Training Fusion Module:  86%|████████▌ | 25/29 [00:02<00:00,  9.76it/s, loss=2.17]Training Fusion Module:  86%|████████▌ | 25/29 [00:02<00:00,  9.76it/s, loss=2.08]Training Fusion Module:  86%|████████▌ | 25/29 [00:02<00:00,  9.76it/s, loss=1.2] Training Fusion Module:  93%|█████████▎| 27/29 [00:02<00:00, 10.24it/s, loss=1.2]Training Fusion Module:  93%|█████████▎| 27/29 [00:02<00:00, 10.24it/s, loss=2.83]Training Fusion Module:  93%|█████████▎| 27/29 [00:02<00:00, 10.24it/s, loss=1.98]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 11.31it/s, loss=1.98]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 10.60it/s, loss=1.98]
2025-10-30 03:23:03,363 - INFO - Train Loss: 2.0628, Train Acc: 36.84%
2025-10-30 03:23:03,363 - INFO - Val Loss: 2.2149, Val Acc: 27.59%
2025-10-30 03:23:03,363 - INFO - 
Epoch 5/5
Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=2.7]Training Fusion Module:   0%|          | 0/29 [00:00<?, ?it/s, loss=1.58]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02,  9.83it/s, loss=1.58]Training Fusion Module:   7%|▋         | 2/29 [00:00<00:02,  9.83it/s, loss=1.49]Training Fusion Module:  10%|█         | 3/29 [00:00<00:02,  9.27it/s, loss=1.49]Training Fusion Module:  10%|█         | 3/29 [00:00<00:02,  9.27it/s, loss=1.79]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  8.95it/s, loss=1.79]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  8.95it/s, loss=1.96]Training Fusion Module:  14%|█▍        | 4/29 [00:00<00:02,  8.95it/s, loss=2.1] Training Fusion Module:  21%|██        | 6/29 [00:00<00:01, 11.84it/s, loss=2.1]Training Fusion Module:  21%|██        | 6/29 [00:00<00:01, 11.84it/s, loss=1.43]Training Fusion Module:  21%|██        | 6/29 [00:00<00:01, 11.84it/s, loss=1.69]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:01, 11.93it/s, loss=1.69]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:01, 11.93it/s, loss=2.02]Training Fusion Module:  28%|██▊       | 8/29 [00:00<00:01, 11.93it/s, loss=2.74]Training Fusion Module:  34%|███▍      | 10/29 [00:00<00:01, 10.35it/s, loss=2.74]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:01, 10.35it/s, loss=0.935]Training Fusion Module:  34%|███▍      | 10/29 [00:01<00:01, 10.35it/s, loss=1.63] Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.56it/s, loss=1.63]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.56it/s, loss=2.43]Training Fusion Module:  41%|████▏     | 12/29 [00:01<00:01,  9.56it/s, loss=1.86]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01,  9.30it/s, loss=1.86]Training Fusion Module:  48%|████▊     | 14/29 [00:01<00:01,  9.30it/s, loss=1.35]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  9.11it/s, loss=1.35]Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  9.11it/s, loss=2.1] Training Fusion Module:  52%|█████▏    | 15/29 [00:01<00:01,  9.11it/s, loss=2.2]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01, 10.00it/s, loss=2.2]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01, 10.00it/s, loss=2.31]Training Fusion Module:  59%|█████▊    | 17/29 [00:01<00:01, 10.00it/s, loss=1.28]Training Fusion Module:  66%|██████▌   | 19/29 [00:01<00:00, 11.10it/s, loss=1.28]Training Fusion Module:  66%|██████▌   | 19/29 [00:01<00:00, 11.10it/s, loss=0.964]Training Fusion Module:  66%|██████▌   | 19/29 [00:02<00:00, 11.10it/s, loss=2.35] Training Fusion Module:  72%|███████▏  | 21/29 [00:02<00:00, 10.56it/s, loss=2.35]Training Fusion Module:  72%|███████▏  | 21/29 [00:02<00:00, 10.56it/s, loss=0.858]Training Fusion Module:  72%|███████▏  | 21/29 [00:02<00:00, 10.56it/s, loss=1.63] Training Fusion Module:  79%|███████▉  | 23/29 [00:02<00:00, 10.81it/s, loss=1.63]Training Fusion Module:  79%|███████▉  | 23/29 [00:02<00:00, 10.81it/s, loss=1.11]Training Fusion Module:  79%|███████▉  | 23/29 [00:02<00:00, 10.81it/s, loss=2.38]Training Fusion Module:  86%|████████▌ | 25/29 [00:02<00:00, 10.60it/s, loss=2.38]Training Fusion Module:  86%|████████▌ | 25/29 [00:02<00:00, 10.60it/s, loss=1.44]Training Fusion Module:  86%|████████▌ | 25/29 [00:02<00:00, 10.60it/s, loss=1.64]Training Fusion Module:  93%|█████████▎| 27/29 [00:02<00:00, 10.66it/s, loss=1.64]Training Fusion Module:  93%|█████████▎| 27/29 [00:02<00:00, 10.66it/s, loss=1.63]Training Fusion Module:  93%|█████████▎| 27/29 [00:02<00:00, 10.66it/s, loss=1.24]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 12.16it/s, loss=1.24]Training Fusion Module: 100%|██████████| 29/29 [00:02<00:00, 10.64it/s, loss=1.24]
2025-10-30 03:23:06,286 - INFO - Train Loss: 1.7528, Train Acc: 43.86%
2025-10-30 03:23:06,286 - INFO - Val Loss: 1.7011, Val Acc: 27.59%
2025-10-30 03:23:06,286 - INFO - 
Fusion Module training complete! Best val_loss: 1.6112
2025-10-30 03:23:06,287 - INFO - 
================================================================================
2025-10-30 03:23:06,287 - INFO - TRAINING COMPLETE!
2025-10-30 03:23:06,287 - INFO - ================================================================================
2025-10-30 03:23:06,287 - INFO - 
Model checkpoints saved to: models/checkpoints
2025-10-30 03:23:06,287 - INFO - 
Next steps:
2025-10-30 03:23:06,287 - INFO - 1. Test your trained models with: python scripts/test_triton.py
2025-10-30 03:23:06,287 - INFO - 2. Compare with baseline results
2025-10-30 03:23:06,287 - INFO - 3. Analyze performance improvements
