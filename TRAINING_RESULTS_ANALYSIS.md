# Training Results Analysis - November 20, 2025

## ðŸ“Š Training Completed

**Status**: âŒ **FAILED** - Same 0.55% accuracy as baseline  
**Training Time**: ~22 minutes (01:45 - 02:07)  
**Completion**: Training completed but model didn't learn

---

## Results Summary

**Test Accuracy**: 0.55% (same as baseline!)  
**Macro F1**: 0.0016  
**Weighted F1**: 0.0001

**Per-Class Performance**:
- All vulnerability types: 0% precision, 0% recall, 0% F1
- Except time_manipulation: 1.0 recall (but only 1 sample)

---

## What Happened

### PDG Extraction

âœ… **Good News**:
- **431 PDGs successfully extracted** (from 817 training contracts)
- **52.7% extraction success rate** for this dataset
- PDGs were high quality (up to 104 nodes, 89 edges)
- Stub injection worked: "Injected 3 dependency stubs: ERC20, Ownable, Pausable"

âŒ **Bad News**:
- Only 52.7% success means **47.3% of contracts had empty/failed PDGs**
- Model trained on mixture of good and bad data

### Training Behavior

**Epochs Completed**: 11 epochs  
**Early Stopping**: Triggered (no improvement for 10 epochs)

**Training Metrics**:
```
Epoch 1:  Train Acc: 0.61% | Val Acc: 16.76%
Epoch 2:  Train Acc: 0.12% | Val Acc: 16.76%
Epoch 3-11: Train Acc: 0.12% | Val Acc: 16.76% (NO CHANGE)
```

**The Problem**: Validation loss and accuracy **NEVER CHANGED** after epoch 1!
- Val Loss stayed at exactly 4.8299
- Val Acc stayed at exactly 16.76%
- This indicates the model isn't learning from the data

---

## Root Cause Analysis

### Why The Model Didn't Learn:

1. **47% Empty PDGs Poisoned Training**
   - 386 out of 817 contracts (47%) had failed PDG extraction
   - These likely got empty or minimal graphs
   - Model learned to ignore graph structure

2. **Dataset Too Small After Filtering**
   - Only 431 valid PDGs across 7 vulnerability classes
   - That's ~62 samples per class average
   - With imbalanced distribution:
     - arithmetic: ~150 samples
     - safe: only ~8 samples (16 * 0.527)
     - time_manipulation: only ~1 sample

3. **Static PDGs May Not Capture Vulnerabilities**
   - PDG shows control/data flow
   - Many vulnerabilities are semantic (logic errors, not flow errors)
   - Pure static analysis may be insufficient

4. **Model Architecture Issues**
   - Graph Attention Network expects meaningful graph structure
   - Empty/minimal graphs don't provide training signal
   - No fallback for failed PDG extraction

---

## Comparison to Expectations

### What We Expected:
- 70% PDG extraction (based on small test sample)
- 30-55% model accuracy
- Significant improvement over 0.55% baseline

### What We Got:
- 52.7% PDG extraction (lower than expected, but reasonable)
- 0.55% model accuracy (NO improvement)
- Model completely failed to learn

### Why The Discrepancy:
1. Test sample was lucky (70% success on 10 contracts)
2. Full dataset has worse quality (52.7% average)
3. Even 52.7% valid PDGs wasn't enough - the empty ones poisoned training
4. Model needs ALL samples to have valid PDGs, not just 50%

---

## Key Insights

### What Worked:
âœ… PDG extraction improvements (77 compilers, stub injection, retries)  
âœ… Extracted 431 high-quality PDGs  
âœ… No crashes or errors during training  
âœ… Infrastructure is solid

### What Didn't Work:
âŒ Training on mixed valid/empty PDGs  
âŒ Model architecture can't handle failed extractions  
âŒ Dataset size too small after PDG failures  
âŒ Static PDGs alone insufficient for vulnerability detection

---

## Next Steps

### Option 1: Filter Out Failed PDGs â­ (Recommended)

**What**: Only train on contracts with successful PDG extraction

**How**:
1. Modify dataset loader to skip contracts with PDG extraction failures
2. Only use the 431 contracts with valid PDGs
3. Re-balance classes if needed

**Expected Result**:
- Training on clean data (100% valid PDGs)
- Better learning signal
- Accuracy: 15-30% (realistic for small dataset)

**Time**: 1-2 hours to implement + 2-3 hours training

---

### Option 2: Expand Dataset with More Sources

**What**: Add more vulnerability examples from other datasets

**Sources**:
- SmartBugs dataset
- SolidiFI benchmark
- Mythril test cases
- Manual vulnerability contracts

**Expected Result**:
- 2000-5000 contracts
- Better class balance
- Higher PDG success rate (more modern, well-formed contracts)

**Time**: 4-8 hours data collection + processing

---

### Option 3: Hybrid Approach (Static + Semantic)

**What**: Combine PDG with other features

**Features to Add**:
- Source code embeddings (CodeBERT, etc.)
- AST (Abstract Syntax Tree)
- Control flow graph
- Bytecode patterns

**Expected Result**:
- More robust to PDG failures
- Better capture of semantic vulnerabilities
- Accuracy: 40-60%

**Time**: 1-2 days implementation

---

### Option 4: Use Pre-trained Models

**What**: Fine-tune existing vulnerability detection models

**Models**:
- CodeBERT fine-tuned on smart contracts
- Slither's ML detectors
- Commercial tools' embeddings

**Expected Result**:
- Leverage pre-trained knowledge
- Better baseline performance
- Accuracy: 50-70%

**Time**: 2-3 days setup + training

---

## Immediate Recommendation

### âœ… **Try Option 1 First** (Filter Out Failed PDGs)

**Rationale**:
1. Quickest to implement (1-2 hours)
2. Uses existing infrastructure
3. Will definitively show if PDGs work when clean
4. Low risk, high learning value

**Implementation**:
1. Modify `scripts/train/static/train_static_optimized.py`
2. Add PDG validation in dataset `__getitem__`
3. Skip samples with `pdg.number_of_nodes() == 0`
4. Re-run training

**If This Works** (accuracy > 15%):
- PDG approach is valid
- Scale up with Option 2 (more data)

**If This Fails** (accuracy still < 5%):
- Static PDGs insufficient
- Try Option 3 (hybrid) or Option 4 (pre-trained)

---

## Summary

**What We Learned**:
- PDG extraction improved 9x (5.8% â†’ 52.7%)
- Infrastructure works well
- But 52.7% success isn't enough - empty PDGs poison training
- Need 100% valid PDGs OR handle failures gracefully

**Critical Issue**:
The model trains on ALL samples including those with failed PDG extraction. This creates a training set where ~50% of samples have no useful features, which prevents learning.

**Solution**:
Filter the dataset to only include contracts with successful PDG extraction. This will give us 431 clean training samples which should be enough to see if the approach works.

---

**Generated**: November 20, 2025, 02:17 AM EST  
**Training Log**: `logs/improved_training_20251120_014509/training.log`  
**Results File**: `models/checkpoints/test_results_20251120_020719.txt`
