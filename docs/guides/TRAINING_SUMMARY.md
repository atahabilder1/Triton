# Static Vulnerability Detection - Complete Summary

## üìÅ Files Created

### Training Scripts
1. **`train_static_optimized.py`** - Optimized training script
   - Uses full GPU power
   - Real-time progress monitoring
   - Early stopping if not improving
   - Detailed per-vulnerability metrics

2. **`start_training_gpu.sh`** - One-command launcher
   - Checks GPU availability
   - Uses optimal settings for A6000
   - Logs everything to file

### Documentation
1. **`TRAINING_QUICK_START.md`** - Quick reference guide
2. **`STATIC_TRAINING_GUIDE.md`** - Detailed explanation

---

## üöÄ How to Start Training

```bash
./start_training_gpu.sh
```

That's it! This will:
- ‚úÖ Use full GPU power (16 batch size, 8 workers)
- ‚úÖ Train for up to 50 epochs (auto-stops if not improving)
- ‚úÖ Show real-time progress every 10 batches
- ‚úÖ Print detailed metrics every 5 epochs
- ‚úÖ Save best model automatically

---

## üìä What You'll See (Real-Time)

### Every 10 Batches:
```
Batch [  10/568] | Loss: 1.2345 | Acc: 45.67% | Speed: 1.23 batch/s | ETA: 7m 32s
```

**How to know if training is working**:
- ‚úÖ Loss decreasing (1.23 ‚Üí 1.15 ‚Üí 1.08 ‚Üí ...)
- ‚úÖ Accuracy increasing (45% ‚Üí 48% ‚Üí 51% ‚Üí ...)
- ‚úÖ Speed consistent (1.2-1.5 batch/s)

### Every Epoch:
```
================================================================================
EPOCH 5/50 SUMMARY
================================================================================
‚è±Ô∏è  Time: 8m 45s
üìâ Train Loss: 1.0234 | Train Acc: 52.34%
üìä Val Loss:   0.9876 | Val Acc:   55.67% | Val F1: 0.5234
‚úÖ NEW BEST MODEL SAVED!
================================================================================
```

### Every 5 Epochs (Detailed Metrics):
```
‚úÖ reentrancy                      0.7234     0.6891     0.7058    94/119  (79.0%)
‚úÖ arithmetic                      0.6912     0.7397     0.7146   108/146  (74.0%)
‚ö†Ô∏è  access_control                 0.5234     0.5891     0.5546    87/148  (58.9%)
‚ùå short_addresses                 0.2286     0.2857     0.2545     2/7    (28.6%)
```

**Symbols**:
- ‚úÖ = Detecting well (>70% recall)
- ‚ö†Ô∏è = Moderate (50-70% recall)
- ‚ùå = Poor (<50% recall)

---

## üõë When to Cancel Training

### Auto-Stop
Training stops automatically if:
```
‚ö†Ô∏è No improvement for 5/5 epochs
üõë EARLY STOPPING: No improvement for 5 epochs
```

### Manual Stop (Ctrl+C)
Stop if you see for 5+ epochs:
- Loss not decreasing
- Accuracy stuck
- F1 score not improving

---

## üí™ GPU Optimization

### Check GPU Usage:
```bash
watch -n 1 nvidia-smi
```

**Should see**:
- GPU Memory: 8-12 GB used
- GPU Utilization: 80-100%
- Power: 250W+ (near max)

### If GPU underutilized:
Edit `start_training_gpu.sh`, increase batch size:
```bash
--batch-size 24  # Instead of 16
```

---

## üìä Dataset Overview

```
FORGE Balanced Dataset: 6,575 contracts
‚îú‚îÄ‚îÄ Train: 4,540 (69%)
‚îú‚îÄ‚îÄ Val:   1,011 (15%)
‚îî‚îÄ‚îÄ Test:  1,024 (16%)

Vulnerability Types (11):
‚úÖ Well-represented (600+ samples):
   - arithmetic: 663
   - unchecked_low_level_calls: 666
   - access_control: 629
   - safe: 606
   - reentrancy: 553
   - other: 620

‚ö†Ô∏è Moderate (100-400 samples):
   - denial_of_service: 317
   - time_manipulation: 206
   - front_running: 138
   - bad_randomness: 112

‚ùå Very imbalanced (<100 samples):
   - short_addresses: 30 (will be hard to detect!)
```

---

## üéØ Expected Results

**Overall Accuracy**: 60-72%

**Best Detected** (using PDG structure):
- Reentrancy: 75-82%
- Arithmetic: 70-77%
- Unchecked Calls: 70-76%

**Challenging** (not visible in PDG):
- Short Addresses: 30-45%
- Bad Randomness: 45-55%
- Front Running: 50-60%

---

## ‚è±Ô∏è Training Time

**GPU (A6000)**:
- First epoch: ~25-30 min (extracts PDGs)
- Later epochs: ~8-10 min (uses cache)
- **Total**: 2-3 hours for 20 epochs

**CPU**:
- First epoch: ~2-3 hours
- Later epochs: ~45-60 min
- **Total**: 15-20 hours for 20 epochs

---

## üìà Monitoring Tools

### TensorBoard (Real-time graphs):
```bash
tensorboard --logdir runs/
```
Open: http://localhost:6006

### Training Log (File):
```bash
tail -f logs/training_gpu_YYYYMMDD_HHMMSS.log
```

### GPU Monitor:
```bash
watch -n 1 nvidia-smi
```

---

## üìÇ Output Files

After training completes:

1. **`models/checkpoints/static_encoder_best.pt`**
   - Best model (highest validation F1)

2. **`models/checkpoints/test_results_*.txt`**
   - Final test metrics (per-vulnerability)

3. **`logs/training_gpu_*.log`**
   - Complete training log

4. **`runs/static_optimized_*/`**
   - TensorBoard logs

---

## üî¨ Understanding the Model

### Input (Intermediate Representation):
```
Smart Contract (.sol)
    ‚Üì
Slither Analysis
    ‚Üì
Program Dependence Graph (PDG)
    ‚îú‚îÄ‚îÄ Nodes: functions, variables, modifiers
    ‚îî‚îÄ‚îÄ Edges: calls, reads, writes, uses_modifier
```

### Model Architecture:
```
PDG ‚Üí Node Encoding (5‚Üí128 dim)
    ‚Üí GAT Layer 1 (128‚Üí256 dim)
    ‚Üí GAT Layer 2 (256‚Üí256 dim)
    ‚Üí GAT Layer 3 (256‚Üí256 dim)
    ‚Üí Global Pooling
    ‚Üí Projection (256‚Üí768 dim)
    ‚Üí 11 Classification Heads
    ‚Üí Vulnerability Prediction
```

### Loss Function:
```python
CrossEntropyLoss(predictions, ground_truth, class_weights)
```

**Purpose**: Measures prediction error
- Low loss (0.2) = good predictions
- High loss (2.5) = poor predictions
- Training minimizes loss via backpropagation

### Class Weights:
Handles imbalanced dataset:
- short_addresses (30 samples): weight = 11.0
- arithmetic (663 samples): weight = 0.5

---

## üêõ Troubleshooting

### Out of Memory
```bash
--batch-size 8  # Reduce from 16
```

### Training Too Slow
```bash
--batch-size 24  # Increase if GPU has memory
--num-workers 16  # Use more CPU cores
```

### Poor Accuracy
Try different learning rate:
```bash
--learning-rate 0.0001  # More stable
# or
--learning-rate 0.01    # Faster convergence
```

### Slither Errors
Check Solidity compiler:
```bash
solc-select use 0.8.0
```

---

## ‚úÖ Quick Checklist

Before training:
- [ ] GPU available (`nvidia-smi` works)
- [ ] Dataset exists (`data/datasets/forge_balanced_accurate/`)
- [ ] Virtual env activated (`source triton_env/bin/activate`)

During training:
- [ ] GPU usage 80-100% (`watch -n 1 nvidia-smi`)
- [ ] Loss decreasing over epochs
- [ ] Accuracy increasing over epochs
- [ ] No warning messages for 5+ epochs

After training:
- [ ] Best model saved (`models/checkpoints/static_encoder_best.pt`)
- [ ] Test results generated (`test_results_*.txt`)
- [ ] Overall accuracy > 60%

---

## üìû Quick Commands Reference

```bash
# Start training
./start_training_gpu.sh

# Monitor GPU
watch -n 1 nvidia-smi

# View TensorBoard
tensorboard --logdir runs/

# Check training log
tail -f logs/training_gpu_*.log

# Stop training
Ctrl+C

# Test trained model
python test_static_model.py  # (create if needed)
```

---

## üéì Key Concepts

**Program Dependence Graph (PDG)**:
- Graph showing relationships in smart contract
- Nodes = functions, variables, modifiers
- Edges = calls, reads, writes
- Captures control/data flow

**Graph Attention Network (GAT)**:
- Neural network for graph data
- Learns important relationships via attention
- 3 layers, 8 attention heads each

**Loss Function**:
- Metric of prediction error
- Backpropagation minimizes loss
- Model learns by adjusting weights

**Class Weights**:
- Handle imbalanced dataset
- Rare classes get higher weights
- Prevents model ignoring minority classes

**Early Stopping**:
- Stops if no improvement for 5 epochs
- Prevents overfitting
- Saves time

---

## üìö Related Documentation

- **TRAINING_QUICK_START.md** - Quick reference
- **STATIC_TRAINING_GUIDE.md** - Detailed guide
- **README.md** - Project overview
- **PROJECT_ORGANIZATION.md** - File structure

---

**Last Updated**: 2025-11-19
