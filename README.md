# Triton: Multi-Modal Smart Contract Vulnerability Detection

Triton is an AI-powered system that detects vulnerabilities in Ethereum smart contracts using:
- **Static Analysis** (Slither): Program Dependence Graphs (PDG)
- **Dynamic Analysis** (Mythril): Execution Traces
- **Semantic Analysis** (CodeBERT): Code Understanding
- **Cross-Modal Fusion**: Combines all three for better accuracy

---

## ğŸš€ Quick Start

### Static Vulnerability Detection (Recommended)

**GPU-Optimized Training:**
```bash
./start_static_training_gpu.sh
```

**Standard Training:**
```bash
./start_static_training.sh
```

**Time:** 2-3 hours (GPU) | 12-15 hours (CPU)

See **[QUICK_START.md](QUICK_START.md)** for detailed guide.

### Full Multi-Modal Pipeline

```bash
./start_full_training.sh
```

**Time:** 8-12 hours (trains all 4 models: static, dynamic, semantic, fusion)

---

## ğŸ“– Documentation

### Quick References
- **[QUICK_START.md](QUICK_START.md)** - Static training quick start
- **[docs/guides/TRAINING_QUICK_START.md](docs/guides/TRAINING_QUICK_START.md)** - Quick reference guide
- **[docs/guides/STATIC_TRAINING_GUIDE.md](docs/guides/STATIC_TRAINING_GUIDE.md)** - Detailed training guide
- **[docs/guides/TRAINING_SUMMARY.md](docs/guides/TRAINING_SUMMARY.md)** - Complete summary

### Detailed Guides
- **[docs/guides/HOW_TO_TRAIN.md](docs/guides/HOW_TO_TRAIN.md)** - Multi-modal training guide
- **[docs/guides/DATASET_AND_TRAINING_SUMMARY.md](docs/guides/DATASET_AND_TRAINING_SUMMARY.md)** - Dataset information
- **[PROJECT_ORGANIZATION.md](PROJECT_ORGANIZATION.md)** - Project structure

---

## ğŸ“Š Expected Performance

**Dataset:** FORGE (6,575 contracts)
- Train: 4,540 contracts (69%)
- Validation: 1,011 contracts (15%)
- Test: 1,024 contracts (16%)

**Static Encoder Detection Rates:**
- Overall Accuracy: 60-72%
- Reentrancy: 75-82% âœ…
- Arithmetic: 70-77% âœ…
- Unchecked Calls: 70-76% âœ…
- Access Control: 60-70% âš ï¸
- Short Addresses: 30-45% âŒ (very imbalanced class)

---

## ğŸ“ Project Structure

```
Triton/
â”œâ”€â”€ scripts/                          # Training & testing scripts
â”‚   â”œâ”€â”€ train_static_optimized.py       # GPU-optimized static training â­
â”‚   â”œâ”€â”€ train_static_only.py            # Standard static training
â”‚   â”œâ”€â”€ train_complete_pipeline.py      # Multi-modal pipeline
â”‚   â”œâ”€â”€ test_dataset_performance.py     # Testing script
â”‚   â”œâ”€â”€ monitor_training_detailed.sh    # Training monitoring
â”‚   â””â”€â”€ quick_status.sh                 # Quick training status
â”‚
â”œâ”€â”€ encoders/                         # Model architectures
â”‚   â”œâ”€â”€ static_encoder.py               # PDG + GAT model
â”‚   â”œâ”€â”€ dynamic_encoder.py              # Execution trace + LSTM
â”‚   â”œâ”€â”€ semantic_encoder.py             # CodeBERT fine-tuning
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ fusion/                           # Cross-modal fusion
â”‚   â””â”€â”€ cross_modal_fusion.py           # Fusion module
â”‚
â”œâ”€â”€ tools/                            # Analysis tools
â”‚   â””â”€â”€ slither_wrapper.py              # PDG extraction (Slither)
â”‚
â”œâ”€â”€ models/checkpoints/               # Trained models
â”‚   â”œâ”€â”€ static_encoder_best.pt          # Best static model
â”‚   â””â”€â”€ ...
â”‚
â”œâ”€â”€ data/datasets/                    # Training data
â”‚   â””â”€â”€ forge_balanced_accurate/        # 6,575 contracts
â”‚       â”œâ”€â”€ train/                      # 4,540 contracts
â”‚       â”œâ”€â”€ val/                        # 1,011 contracts
â”‚       â””â”€â”€ test/                       # 1,024 contracts
â”‚
â”œâ”€â”€ docs/guides/                      # Documentation
â”‚   â”œâ”€â”€ TRAINING_QUICK_START.md         # Quick reference
â”‚   â”œâ”€â”€ STATIC_TRAINING_GUIDE.md        # Detailed guide
â”‚   â”œâ”€â”€ TRAINING_SUMMARY.md             # Complete summary
â”‚   â”œâ”€â”€ DATASET_AND_TRAINING_SUMMARY.md # Dataset info
â”‚   â””â”€â”€ HOW_TO_TRAIN.md                 # Multi-modal guide
â”‚
â”œâ”€â”€ logs/                             # Training logs
â”œâ”€â”€ runs/                             # TensorBoard logs
â”œâ”€â”€ results/                          # Test results
â”‚
â”œâ”€â”€ start_static_training_gpu.sh      # Static GPU-optimized launcher â­
â”œâ”€â”€ start_static_training.sh          # Static standard launcher
â”œâ”€â”€ start_full_training.sh            # Full pipeline launcher (all 4 models)
â”œâ”€â”€ QUICK_START.md                    # Quick start guide
â””â”€â”€ README.md                         # This file
```

---

## ğŸ› ï¸ Requirements

- Python 3.8+
- PyTorch
- Transformers (HuggingFace)
- Slither
- Mythril
- PyTorch Geometric

Install:
```bash
pip install -r requirements.txt
```

---

## ğŸ¯ What Triton Does

1. **Loads** smart contracts from dataset
2. **Extracts** features using 3 analysis methods:
   - **Static**: Program Dependence Graphs (Slither)
   - **Dynamic**: Execution Traces (Mythril)
   - **Semantic**: Code Embeddings (CodeBERT)
3. **Trains** neural network components:
   - Static: Graph Attention Network (GAT)
   - Dynamic: LSTM
   - Semantic: Fine-tuned Transformer
   - Fusion: Cross-modal attention
4. **Detects** 11 vulnerability types:
   - Reentrancy
   - Arithmetic Overflow/Underflow
   - Access Control
   - Unchecked Low-Level Calls
   - Bad Randomness
   - Denial of Service
   - Front Running
   - Time Manipulation
   - Short Address Attack
   - Other
   - Safe (no vulnerabilities)

---

## ğŸ“š Training Modes

### Static-Only (Recommended)
- **Time:** 2-3 hours (GPU)
- **Model:** Graph Attention Network on PDGs
- **Accuracy:** 60-72%
- **Best for:** Control flow vulnerabilities (reentrancy, arithmetic, etc.)

### Full Multi-Modal Pipeline
- **Time:** 8-12 hours (GPU)
- **Models:** All 4 components (static, dynamic, semantic, fusion)
- **Accuracy:** 55-70% (fusion)
- **Best for:** Comprehensive detection

---

## ğŸ”¬ Real-Time Training Monitoring

During training, you'll see:

**Every 10 batches:**
```
Batch [  10/568] | Loss: 1.2345 | Acc: 45.67% | Speed: 1.23 batch/s | ETA: 7m 32s
```

**Every epoch:**
```
EPOCH 5/50 SUMMARY
Train Loss: 1.0234 | Train Acc: 52.34%
Val Loss:   0.9876 | Val Acc:   55.67% | Val F1: 0.5234
âœ… NEW BEST MODEL SAVED!
```

**Every 5 epochs (detailed metrics):**
```
âœ… reentrancy           0.7234  0.6891  0.7058   94/119 (79.0%)
âš ï¸  access_control      0.5234  0.5891  0.5546   87/148 (58.9%)
âŒ short_addresses      0.2286  0.2857  0.2545    2/7   (28.6%)
```

See `docs/guides/TRAINING_QUICK_START.md` for interpretation.

---

## ğŸ’ª GPU Optimization

The training scripts are optimized for RTX A6000 (46GB VRAM):
- Batch size: 16 (adjust for your GPU)
- Parallel data loading: 8 workers
- Mixed precision training
- Automatic early stopping

**Monitor GPU usage:**
```bash
watch -n 1 nvidia-smi
```

**Expected:** 80-100% GPU utilization, 8-12 GB memory usage

---

## ğŸ“„ License

MIT License

---

**Last Updated:** November 19, 2025
